{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML with TPOT\n",
    "\n",
    "Automated machine learning holds much promise as an assistive tool for practitioners working with data. In this notebook, we demonstrate the [TPOT](http://epistasislab.github.io/tpot/) library for AutoML running across a [Dask](https://dask.org/) cluster, which we distribute using the CML [Workers API](https://docs.cloudera.com/machine-learning/cloud/distributed-computing/topics/ml-parallel-computing.html).\n",
    "\n",
    "We'll work with a small sample of the [credit card fraud dataset](https://www.kaggle.com/mlg-ulb/creditcardfraud) curated by the machine learning group at Universit√© Libre de Bruxelles. The same group have a [handbook](https://fraud-detection-handbook.github.io/fraud-detection-handbook/Foreword.html) detailing the application of machine learning for fraud detection, which provides considerably more depth than we do here. Creating a realistic fraud detection system is far too mighty a task for a single humble notebook, and so the dataset here serves only as an example for the purpose of demonstrating the technologies. Namely: TPOT running in parallel on a Dask cluster on CML.\n",
    "\n",
    "We'll first load the data and perform some automated data profiling. Then, we'll spin up a Dask cluster on top of our CML cluster, on the fly. We'll define a TPOT classifier, fit it to our data, and then shut down our cluster to free up compute resources for other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We import all dependencies up front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import cdsw\n",
    "import pandas as pd\n",
    "\n",
    "from dask.distributed import Client\n",
    "from pandas_profiling import ProfileReport\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tpot import TPOTClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and profile the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.read_csv(\"data/creditcardsample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's profile the data. Exploratory data analysis is not a mechanical process, and thus impossible to automate completely. However, open source provides us several options to automatically generate common charts. Here, we're using [pandas-profiling](https://github.com/pandas-profiling/pandas-profiling) to view histograms of each variable and detect duplicate entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProfileReport(transactions, interactions=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values, but there are some duplicate rows. We don't want to accidentally polute our test set with rows that are duplicated in the training set, so we'll drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_transactions = transactions.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = unique_transactions.drop([\"Class\", \"Time\"], axis=\"columns\")\n",
    "y = unique_transactions.Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split our data into train and test sets, so we can evaluate performance on a hold out set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask cluster\n",
    "\n",
    "Training a TPOT classifier involves training many possible pipelines to learn which preprocessing steps, algorithms, and hyperparameter combinations perform best on a particular data set. The algorithm is partly parallelizable, and so we benefit from being able to train multiple pipelines at once. TPOT comes with Dask integration, making distributing the work easy, so long as there is a Dask cluster. Here, we spin up an ad hoc cluster by launching multiple CML sessions using the CML workers API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Dask scheduler\n",
    "\n",
    "We need to make two directories required by Dask. Dask uses these directories to share network information between the scheduler and workers. From our, user, perspective, we can create them and forget them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"_scheduler_\", exist_ok=True)\n",
    "os.makedirs(\"_worker_\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start a Dask scheduler as a CDSW worker process. We do this with `cdsw.launch_workers`, which spins up another session on our cluster and runs the command we provide &mdash; in this case the Dask scheduler. The scheduler is responsible for coordinating work between the Dask workers we will attach. Later we'll start a Dask client in this notebook. The client talks to the scheduler, and the scheduler talks to the workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_scheduler = cdsw.launch_workers(\n",
    "  n=1,\n",
    "  cpu=1,\n",
    "  memory=2,\n",
    "  code=f\"!dask-scheduler --host 0.0.0.0 --dashboard-address 127.0.0.1:8090 --scheduler-file /home/cdsw/_scheduler_/dask.log\"\n",
    ")\n",
    "\n",
    "# Wait for the scheduler to start.\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the IP address of the CML worker with the scheduler on it, so we can connect the Dask workers to it. The IP is not returned in the `dask_scheduler` object (it's unknown at the launch of the scheduler), so we scan through the worker list and find the IP of the worker with the scheduler id. This returns a list, but there should be only one entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_workers = cdsw.list_workers()\n",
    "scheduler_id = dask_scheduler[0]['id']\n",
    "scheduler_ip = [worker['ip_address'] for worker in scheduler_workers\n",
    "                if worker['id'] == scheduler_id][0]\n",
    "\n",
    "scheduler_url = f\"tcp://{scheduler_ip}:8786\"\n",
    "\n",
    "scheduler_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Dask workers\n",
    "\n",
    "We're ready to grow our cluster. We start some more CML workers, each with one Dask worker process on it. We pass the scheduler URL we just found so that the scheduler can talk, and distribute work, to the workers.\n",
    "\n",
    "`N_WORKERS` determines the number of CML workers started (and thus the number of Dask workers running in those sessions). Increasing the number will start more workers. This will speed up the wall-clock time of the TPOT training process, by training more pipelines in parallel, but it uses more cluster resources. Exercise good judgement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WORKERS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_workers = cdsw.launch_workers(\n",
    "  n=N_WORKERS,\n",
    "  cpu=1,\n",
    "  memory=2,\n",
    "  code=f\"!dask-worker {scheduler_url} --local-directory /home/cdsw/_worker_\"\n",
    ")\n",
    "\n",
    "# Wait for the workers to start.\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect Dask client\n",
    "\n",
    "We have a Dask cluster running and distributed over CML sessions. Now we can start a local Dask client and connect it to our scheduler. This is the connection that lets us issue instructions to the Dask cluster. Below, we'll let TPOT handle sending the instructions to Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(scheduler_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view some stats about the Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dask scheduler hosts a dashboard so we can monitor the work it's doing. Here we construct the URL of dashboard, which is hosted on the scheduler worker. Clicking it should open the dashboard in a new browser window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"//\".join(dask_scheduler[0][\"app_url\"].split(\"//\"))+ \"status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's our Dask cluster set up and ready to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a model with TPOT\n",
    "\n",
    "### Define an estimator\n",
    "\n",
    "We define a TPOT classifier. TPOT is rather sophisticated, and will search over many possible pipelines of sklearn preprocessors and estimators. All we have to do to use the Dask cluster is pass the `use_dask=True` flag, and it'll connect via the client we defined (we do not need to (and cannot) explicitly pass the client).\n",
    "\n",
    "Our dataset is highly class-imbalanced (there are few fraudulent transactions), so we choose [average precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html) as an approriate metric, since it is a classification-threshold-agnostic characterization of the precision-recall curve. In reality, there may be a more appropriate metric dictated by the eventual use of this algorithm in the context of a broader fraud detection system (for instance, perhaps this is a first pass filter).\n",
    "\n",
    "TPOT operates on sequential generations of candidate pipelines. It tries `population_size` pipeline combinations, then collects the results, and chooses new combinations in a smart way (it's an evolutionary algorithm). It repeats this `generations` number of times. For each pipeline, it uses 10-fold cross-validation. This is a lot of compute (to do it properly, expect hours or days), so we have restricted to a mere 5 generations, each with population 20, for the sake of this demo notebook. We can stop the process at any point, and TPOT will output the best performing pipeline to that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TPOTClassifier(scoring=\"average_precision\", generations=5, population_size=20, use_dask=True, verbosity=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the estimator\n",
    "\n",
    "We're ready to fit the `TPOTClassifier`. With all the spec given above, this is a small line of code to kick call to do a whole lot of compute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this object exactly like a sklearn estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the estimator directly. Alternatively, we can exporting it, to generate a short template Python script which builds the selected pipeline from it's raw scikit-learn components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.export(\"tpot_estimator.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "We should be good tenants of the cluster and stop hogging workers we aren't using. We should stop only those workers that we started, not all the workers on the cluster, that others may be using. It's also possible to do this through the CML UI in the Sessions view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdsw.stop_workers(*[worker[\"id\"] for worker in dask_workers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also stop the worker hosting the Dask scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdsw.stop_workers(*[worker['id'] for worker in dask_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, we're done cleaning up. We hope this helps seed your own experiments in automated machine learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***If this documentation includes code, including but not limited to, code examples, Cloudera makes this available to you under the terms of the Apache License, Version 2.0, including any required notices. A copy of the Apache License Version 2.0 can be found [here](https://opensource.org/licenses/Apache-2.0).***"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47e3f06f604185201defce9069871b6b92234382a2d95232b3e6dad9ebdc1328"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
